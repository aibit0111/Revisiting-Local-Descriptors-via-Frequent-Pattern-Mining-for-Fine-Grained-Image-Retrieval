{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from skimage.measure import regionprops\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def global_feature_learning(image):\n",
    "    # Load a pretrained VGG16 model\n",
    "    cnn = models.vgg16(pretrained=True).features.eval()\n",
    "\n",
    "    # Define the image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Apply the transformations to the image\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Feed the image into the CNN\n",
    "    T = cnn(image)\n",
    "    T = T.unsqueeze(0)\n",
    "    h, w, d = T.shape\n",
    "\n",
    "    # Add up the activated tensor through the d direction\n",
    "    A = T.sum(dim=1)\n",
    "\n",
    "    # Calculate the mean value of A as the threshold\n",
    "    delta = A.mean()\n",
    "\n",
    "    # Generate the mask map A0\n",
    "    A0 = (A > delta).float()\n",
    "\n",
    "    # Retain the largest connected component\n",
    "    A0 = retain_largest_connected_component(A0)\n",
    "\n",
    "    # Learn the global feature fG\n",
    "    fG = F.adaptive_avg_pool2d(A0, (1, 1)).view(1, -1)\n",
    "\n",
    "    return fG\n",
    "\n",
    "def retain_largest_connected_component(A0):\n",
    "    # Implement the flood fill algorithm to retain the largest connected component\n",
    "    labels = measure.label(A0.cpu().numpy())\n",
    "    largest_label = labels.max()\n",
    "    if largest_label > 0:\n",
    "        largest_area = 0\n",
    "        for i in range(1, largest_label + 1):\n",
    "            area = (labels == i).sum()\n",
    "            if area > largest_area:\n",
    "                largest_area = area\n",
    "                largest_component = i\n",
    "        A0[labels != largest_component] = 0\n",
    "    return A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image using PIL\n",
    "image_path = r'C:\\Users\\mohit\\OneDrive\\Desktop\\remove_background\\entropy\\CUB_200_2011\\images\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.jpg'  # Replace with your image path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image file\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m global_feature_learning(image)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mglobal_feature_learning\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     17\u001b[0m T \u001b[39m=\u001b[39m cnn(image)\n\u001b[0;32m     18\u001b[0m T \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m h, w, d \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mshape\n\u001b[0;32m     21\u001b[0m \u001b[39m# Add up the activated tensor through the d direction\u001b[39;00m\n\u001b[0;32m     22\u001b[0m A \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "global_feature_learning(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_images = []\n",
    "database_filenames = []\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_image_folder = r\"C:\\Users\\mohit\\OneDrive\\Desktop\\remove_background\\entropy\\CUB_200_2011\\images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: 'C:\\\\Users\\\\mohit\\\\OneDrive\\\\Desktop\\\\remove_background\\\\entropy\\\\CUB_200_2011\\\\images\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m database_filenames\u001b[39m.\u001b[39mappend(filename)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Open and transform the image, then append it to the images list\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mwith\u001b[39;00m Image\u001b[39m.\u001b[39;49mopen(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(subdir, filename)) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m     13\u001b[0m     image \u001b[39m=\u001b[39m transform(image)\n\u001b[0;32m     14\u001b[0m     database_images\u001b[39m.\u001b[39mappend(image)\n",
      "File \u001b[1;32mc:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 24] Too many open files: 'C:\\\\Users\\\\mohit\\\\OneDrive\\\\Desktop\\\\remove_background\\\\entropy\\\\CUB_200_2011\\\\images\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define image extensions\n",
    "image_extensions = ['jpg', 'png', 'gif', 'jpeg']\n",
    "\n",
    "# Iterate over all subdirectories and files in the root directory\n",
    "for subdir, dirs, files in os.walk(database_image_folder):\n",
    "    for filename in files:\n",
    "        # Check if the file is an image\n",
    "        if filename.split('.')[-1].lower() in image_extensions:\n",
    "            # Append the filename to the filenames list\n",
    "            database_filenames.append(filename)\n",
    "            # Open and transform the image, then append it to the images list\n",
    "            with Image.open(os.path.join(subdir, filename)) as image:\n",
    "                image = transform(image)\n",
    "                database_images.append(image)\n",
    "\n",
    "# Convert the list of images into a torch tensor\n",
    "database_images = torch.stack(database_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: 'C:\\\\Users\\\\mohit\\\\OneDrive\\\\Desktop\\\\remove_background\\\\entropy\\\\CUB_200_2011\\\\images\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m             image\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m image_names,database_images\n\u001b[1;32m---> 19\u001b[0m image_names,database_images \u001b[39m=\u001b[39m get_image_names_from_folder(database_image_folder)\n",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m, in \u001b[0;36mget_image_names_from_folder\u001b[1;34m(root_folder)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m image_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(path)\n\u001b[1;32m---> 11\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(path)\n\u001b[0;32m     12\u001b[0m keep \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     13\u001b[0m database_images\u001b[39m.\u001b[39mappend(keep)\n",
      "File \u001b[1;32mc:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 24] Too many open files: 'C:\\\\Users\\\\mohit\\\\OneDrive\\\\Desktop\\\\remove_background\\\\entropy\\\\CUB_200_2011\\\\images\\\\001.Black_footed_Albatross\\\\Black_Footed_Albatross_0001_796111.jpg'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "def get_image_names_from_folder(root_folder):\n",
    "    image_extensions = ['jpg', 'png', 'gif', 'jpeg']\n",
    "    image_names = []\n",
    "    database_images = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths = glob.glob(f'{root_folder}/**/*.{ext}', recursive=True)\n",
    "        for path in image_paths:\n",
    "            print(1)\n",
    "            image_name = os.path.basename(path)\n",
    "            image = Image.open(path)\n",
    "            keep = image.copy()\n",
    "            database_images.append(keep)\n",
    "            image_names.append(image_name)\n",
    "            image.close()\n",
    "\n",
    "    return image_names,database_images\n",
    "\n",
    "image_names,database_images = get_image_names_from_folder(database_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11788"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
