{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from skimage.measure import regionprops\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def global_feature_learning(image):\n",
    "    # Load a pretrained VGG16 model\n",
    "    cnn = models.vgg16(pretrained=True).features.eval()\n",
    "\n",
    "    # Define the image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Apply the transformations to the image\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Feed the image into the CNN\n",
    "    T = cnn(image)\n",
    "    T = T.unsqueeze(0)\n",
    "    h, w, d = T.shape\n",
    "\n",
    "    # Add up the activated tensor through the d direction\n",
    "    A = T.sum(dim=1)\n",
    "\n",
    "    # Calculate the mean value of A as the threshold\n",
    "    delta = A.mean()\n",
    "\n",
    "    # Generate the mask map A0\n",
    "    A0 = (A > delta).float()\n",
    "\n",
    "    # Retain the largest connected component\n",
    "    A0 = retain_largest_connected_component(A0)\n",
    "\n",
    "    # Learn the global feature fG\n",
    "    fG = F.adaptive_avg_pool2d(A0, (1, 1)).view(1, -1)\n",
    "\n",
    "    return fG\n",
    "\n",
    "def retain_largest_connected_component(A0):\n",
    "    # Implement the flood fill algorithm to retain the largest connected component\n",
    "    labels = measure.label(A0.cpu().numpy())\n",
    "    largest_label = labels.max()\n",
    "    if largest_label > 0:\n",
    "        largest_area = 0\n",
    "        for i in range(1, largest_label + 1):\n",
    "            area = (labels == i).sum()\n",
    "            if area > largest_area:\n",
    "                largest_area = area\n",
    "                largest_component = i\n",
    "        A0[labels != largest_component] = 0\n",
    "    return A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image using PIL\n",
    "image_path = r'C:\\Users\\mohit\\OneDrive\\Desktop\\remove_background\\entropy\\CUB_200_2011\\images\\001.Black_footed_Albatross\\Black_Footed_Albatross_0001_796111.jpg'  # Replace with your image path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image file\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mohit\\.conda\\envs\\pytorch_1\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m global_feature_learning(image)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mglobal_feature_learning\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     17\u001b[0m T \u001b[39m=\u001b[39m cnn(image)\n\u001b[0;32m     18\u001b[0m T \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m h, w, d \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mshape\n\u001b[0;32m     21\u001b[0m \u001b[39m# Add up the activated tensor through the d direction\u001b[39;00m\n\u001b[0;32m     22\u001b[0m A \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "global_feature_learning(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
