{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This file take random \"N\" number of query and and relevant images will from that random query class, \n",
        "# Only we need to give the databse path\n",
        "# And hyperparameter"
      ],
      "metadata": {
        "id": "Ky_3kfo9AvK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PJTgoqAu9IsL",
        "outputId": "67cfd1d8-4ec1-47d3-9f5c-fdd67d1105ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "0znW3nPVj7gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "id": "MsUUN6vmkCHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "DGn_alwNkGtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend==0.17.0"
      ],
      "metadata": {
        "id": "5-dWIpXIkh4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d1b375-3e67-4727-f09d-dfd7ab0d2a9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlxtend==0.17.0 in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (3.7.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend==0.17.0) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend==0.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend==0.17.0) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.3->mlxtend==0.17.0) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend==0.17.0) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "waO9A1x0kl3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image"
      ],
      "metadata": {
        "id": "2TQi6rzokzJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torchvision.models import vgg16\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from skimage import measure\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n"
      ],
      "metadata": {
        "id": "z2Ne630cBVRU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper_parameter\n",
        "\n",
        "no_random_images = 2 # random number of images N\n",
        "\n",
        "top_no_image_print = 6  # What number of top images to be shown in image, Note it should be less than K\n",
        "\n",
        "alpha = 0.1 # α is used for balancing the effect of global and local features, Range - {0, 0.01, 0.1, · · · , 100}\n",
        "\n",
        "k = 5 # Get top K images\n",
        "\n",
        "minsupp = 2 # minimum support threshold for FPM, The range of minsupp are {0, 1, 2, · · · , 10}"
      ],
      "metadata": {
        "id": "3LKlXxlp8FKN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define paths  database image folders\n",
        "database_image_folder = r\"/content/drive/MyDrive/Colab Notebooks/database_new\"\n"
      ],
      "metadata": {
        "id": "OtqzknyYBjuh"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GlobalFeature:\n",
        "    def __init__(self, vgg16):\n",
        "        self.vgg16 = vgg16\n",
        "\n",
        "    def extract(self, image):\n",
        "        with torch.no_grad():\n",
        "            features = self.vgg16.features(image)\n",
        "\n",
        "        # Obtain the salient object by performing a mask operation\n",
        "        features = features.squeeze(0)\n",
        "        A = features.sum(dim=0)\n",
        "        threshold = A.mean()\n",
        "        mask = (A > threshold).float()\n",
        "\n",
        "        # Retain the largest connected component using the flood fill algorithm\n",
        "        labels = measure.label(mask.cpu().numpy())\n",
        "        largest_label = labels.max()\n",
        "        if largest_label > 0:\n",
        "            largest_area = 0\n",
        "            for i in range(1, largest_label + 1):\n",
        "                area = (labels == i).sum()\n",
        "                if area > largest_area:\n",
        "                    largest_area = area\n",
        "                    largest_component = i\n",
        "\n",
        "        for i in range(mask.shape[0]):\n",
        "            for j in range(mask.shape[1]):\n",
        "                if labels[i, j] != largest_component:\n",
        "                    mask[i, j] = 0\n",
        "\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "        salient_object = features * mask\n",
        "\n",
        "        # Extract the global feature fG from the salient object\n",
        "        fG_max_pooling = nn.functional.adaptive_max_pool2d(salient_object, (1, 1)).view(-1)\n",
        "        fG_avg_pooling = nn.functional.adaptive_avg_pool2d(salient_object, (1, 1)).view(-1)\n",
        "        fG = torch.cat((fG_max_pooling, fG_avg_pooling), dim=0)\n",
        "\n",
        "        return fG"
      ],
      "metadata": {
        "id": "a7o80s4Ejkmf"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LocalFeature:\n",
        "    def __init__(self, vgg16):\n",
        "        self.vgg16 = vgg16\n",
        "\n",
        "    def extract(self, image):\n",
        "        with torch.no_grad():\n",
        "            features = self.vgg16.features(image)\n",
        "\n",
        "        # Obtain the salient object by performing a mask operation\n",
        "        features = features.squeeze(0)\n",
        "        A = features.sum(dim=0)\n",
        "        threshold = A.mean()\n",
        "        mask = (A > threshold).float()\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "        salient_object = features * mask\n",
        "        salient_object = salient_object.squeeze(0)\n",
        "\n",
        "        # Convert feature maps and activated positions into transactions and items\n",
        "        transactions = []\n",
        "        for i in range(salient_object.shape[0]):\n",
        "            feature_map = salient_object[i]\n",
        "            activated_positions = (feature_map > 0).nonzero(as_tuple=True)\n",
        "            items = [f'({x},{y})' for x, y in zip(*activated_positions)]\n",
        "            transactions.append(items)\n",
        "\n",
        "        # Mine frequent patterns using FPM\n",
        "        \n",
        "        I = sorted(set(item for transaction in transactions for item in transaction))\n",
        "        df = pd.DataFrame([[int(item in transaction) for item in I] for transaction in transactions], columns=I)\n",
        "        frequent_itemsets = fpgrowth(df, min_support=minsupp/len(transactions), use_colnames=True)\n",
        "\n",
        "        # Extract the local feature fL from the frequent patterns\n",
        "        patterns = torch.zeros_like(salient_object)\n",
        "        for itemset in frequent_itemsets['itemsets']:\n",
        "            for item in itemset:\n",
        "                x, y = map(int, item.strip('()').split(','))\n",
        "                patterns[:, x, y] = 1\n",
        "\n",
        "        fL_max_pooling = nn.functional.adaptive_max_pool2d(patterns, (1, 1)).view(-1)\n",
        "        fL_avg_pooling = nn.functional.adaptive_avg_pool2d(patterns, (1, 1)).view(-1)\n",
        "        fL = torch.cat((fL_max_pooling, fL_avg_pooling), dim=0)\n",
        "\n",
        "        return fL"
      ],
      "metadata": {
        "id": "mR3iarhnjsF7"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_score(feature1, feature2):\n",
        "    score = torch.dot(feature1, feature2) / (torch.norm(feature1) * torch.norm(feature2))\n",
        "    return score.item()"
      ],
      "metadata": {
        "id": "qmoiV65A6Jkc"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision(retrieved_items, relevant_items):\n",
        "    rel_count = 0\n",
        "    precisions = []\n",
        "\n",
        "    for i, item in enumerate(retrieved_items, start=1):\n",
        "        if item in relevant_items:\n",
        "            rel_count += 1\n",
        "            precision_at_i = rel_count / i\n",
        "            precisions.append(precision_at_i)\n",
        "\n",
        "    if precisions:\n",
        "        avg_precision = sum(precisions) / len(precisions)\n",
        "    else:\n",
        "        avg_precision = 0.0\n",
        "\n",
        "    return avg_precision"
      ],
      "metadata": {
        "id": "zJ4om6Bz6LuW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(ap_scores):\n",
        "    map_score = sum(ap_scores) / len(ap_scores)\n",
        "    return map_score"
      ],
      "metadata": {
        "id": "wgHML0Pc6PtA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model = vgg16(pretrained=True)\n"
      ],
      "metadata": {
        "id": "rUfO1i7F6lhy"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Lambda(lambda image: image.convert('RGB') if image.mode != 'RGB' else image),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "QCFjsymQ6fNI"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_transform_images(database_image_folder, transform):\n",
        "    # Define image extensions\n",
        "    image_extensions = ['jpg', 'png', 'jpeg']\n",
        "\n",
        "    database_filenames = []\n",
        "    database_images = []\n",
        "    database_file_path = []\n",
        "\n",
        "    # Iterate over all subdirectories and files in the root directory\n",
        "    for subdir, dirs, files in os.walk(database_image_folder):\n",
        "        for filename in files:\n",
        "            # Check if the file is an image\n",
        "            if filename.split('.')[-1].lower() in image_extensions:\n",
        "                # Append the filename to the filenames list\n",
        "                database_filenames.append(filename)\n",
        "                # Open and transform the image, then append it to the images list\n",
        "                file_path = os.path.join(subdir, filename)\n",
        "                database_file_path.append(file_path)\n",
        "                with Image.open(os.path.join(subdir, filename)) as image:\n",
        "                    try:\n",
        "                        tensor_image = transform(image)\n",
        "                        database_images.append(tensor_image)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error transforming image {filename}: {str(e)}\")\n",
        "\n",
        "    # Convert the list of images into a torch tensor\n",
        "    try:\n",
        "        database_images = torch.stack(database_images)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in stacking images: {str(e)}\")\n",
        "\n",
        "    return database_filenames, database_images, database_file_path\n",
        "\n",
        "database_filenames, database_images, database_file_path  = load_and_transform_images(database_image_folder, transform)\n"
      ],
      "metadata": {
        "id": "GUQO6lcGBuR6"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_random_image(number_of_random_images, database_filenames, database_images, database_file_path):\n",
        "    \n",
        "    all_average_precision = []\n",
        "\n",
        "    for i_random in range(number_of_random_images):\n",
        "\n",
        "        query_image_path = random.choice(database_file_path)\n",
        "        relevant_image_folder = os.path.dirname(query_image_path)\n",
        "        query_image = Image.open(query_image_path)\n",
        "        query_image = transform(query_image)\n",
        "\n",
        "        def get_image_names_from_folder(root_folder):\n",
        "            image_extensions = ['jpg', 'png', 'gif', 'jpeg']\n",
        "            image_names = []\n",
        "\n",
        "            for ext in image_extensions:\n",
        "                image_paths = glob.glob(f'{root_folder}/**/*.{ext}', recursive=True)\n",
        "                for path in image_paths:\n",
        "                    image_name = os.path.basename(path)\n",
        "                    image_names.append(image_name)\n",
        "\n",
        "            return image_names\n",
        "\n",
        "        relevant_images = get_image_names_from_folder(relevant_image_folder)\n",
        "\n",
        "\n",
        "        \n",
        "        global_extractor = GlobalFeature(vgg16_model)\n",
        "        query_global_feature = global_extractor.extract(query_image)\n",
        "        local_extractor = LocalFeature(vgg16_model)\n",
        "        query_local_feature = global_extractor.extract(query_image)\n",
        "\n",
        "        query_features = query_global_feature + alpha * query_local_feature\n",
        "\n",
        "\n",
        "        all_similarity_score = []\n",
        "\n",
        "        for i,j in zip(database_filenames, database_images):\n",
        "            file_name = i\n",
        "            image = j\n",
        "            global_feature = global_extractor.extract(image)\n",
        "            local_feature = global_extractor.extract(image)\n",
        "            database_feature =  global_feature + alpha * local_feature\n",
        "            get_similarity = similarity_score(database_feature, query_features)\n",
        "            all_similarity_score.append(get_similarity)\n",
        "\n",
        "\n",
        "        \n",
        "        def get_top_k_indices(input_list, k):\n",
        "            return sorted(range(len(input_list)), key=lambda i: input_list[i], reverse=True)[:k]\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        top_k_indices = get_top_k_indices(all_similarity_score, k)\n",
        "\n",
        "        top_k_results_path = [database_file_path[indices]  for indices in top_k_indices]\n",
        "        top_k_results = [database_filenames[indices]  for indices in top_k_indices]\n",
        "\n",
        "        one_average_precision = average_precision(top_k_results, relevant_images)\n",
        "\n",
        "\n",
        "\n",
        "        all_average_precision.append(one_average_precision)\n",
        "\n",
        "\n",
        "\n",
        "        # query image\n",
        "        print(\"\\n\")\n",
        "        print(\"Random Image Number -> \", i_random+1)\n",
        "\n",
        "        img = Image.open(query_image_path)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Query Images\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Average precision for this image\", one_average_precision)\n",
        "\n",
        "\n",
        "                # Open and display each image\n",
        "        for i in range(min(top_no_image_print, k)):\n",
        "            path = top_k_results_path[i]\n",
        "            img = Image.open(path)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f'Top {i+1} result')  # Add a title to the image\n",
        "            plt.show()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"_________________________________________________________________________________________________________________________________________\")\n",
        "        print(\"\\n\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return all_average_precision\n"
      ],
      "metadata": {
        "id": "t5aZgKs7BzEM"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\n",
        "ap = get_random_image(no_random_images,database_filenames, database_images, database_file_path )\n",
        "\n"
      ],
      "metadata": {
        "id": "3dX18O7sB1Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_ = mean_average_precision(ap)\n",
        "\n",
        "print(map_)"
      ],
      "metadata": {
        "id": "94cp2UViCbcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db8f76d-34a8-4c21-c8cb-6c5317bced06"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5895833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tEO4IJXw1q-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}